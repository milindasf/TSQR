\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{pgfplots}
\usetikzlibrary{patterns}

\makeatletter
\newcommand\resetstackedplots{
\makeatletter
\pgfplots@stacked@isfirstplottrue
\makeatother
\addplot [forget plot,draw=none] coordinates{(10K,0) (100K,0) (1000K,0)};
}
\makeatother

\makeatletter
\newcommand\resetstackedplotsOne{
\makeatletter
\pgfplots@stacked@isfirstplottrue
\makeatother
\addplot [forget plot,draw=none] coordinates{(1,0) (2,0) (4,0) (8,0) (16,0)};
}
\makeatother


\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\newcommand{\R}{\mathbb{R}}

\title{Parla Performance Study}
%\author{Milinda Fernando and Geroge Biros}

\begin{document}
\maketitle

\section{Benchmark} 
We use the tall skinny QR (TSQR) decomposition algorithm proposed in \cite{benson2013direct,demmel2008communication}. For a given matrix $A\in \R^{m\times n}$ we assume $m>>n$. The parallel TSQR algorithm can be summarized as follows. 

\begin{algorithm}
    \caption{TSQR decomposition}
    \begin{algorithmic} 
    \REQUIRE $A \in \R^{m\times n}, m>>n$, $p$ parallel tasks
    \ENSURE $A=QR$ for $Q\in \R^{m\times n }$, $Q^TQ=I$ and $R$ is upper triangular.
    \STATE $Ar \leftarrow Partition(A,p)$
    \STATE $Q1r,R1r \leftarrow QR(Ar)$
    \STATE $R1 \leftarrow Gather(R1r,p)$
    \STATE $Q2,R \leftarrow Qr(R1)$
    \STATE $Q2r \leftarrow Partition(Q2r)$ \COMMENT{Same parition bounds as $A$}
    \STATE $Q\leftarrow Q1r \times Q2r$
    \RETURN $Q,R$
    \end{algorithmic}
\end{algorithm}

\section{Implementation}
The following 3 implementations are considered in this performance evaluation. 
\begin{itemize}
    \item Parla QR + Cupy
    \item Python thread pool execution + Cupy
    \item Python MPI + Cupy
\end{itemize}

\section{Experimental setup}
\begin{itemize}
    \item The experiments are conducted in Frontera, GPU cluster, where a single node consists of Two Intel Xeon E5-2620 v4 (“Broadwell”) with four NVIDIA Quadro RTX5000 GPUs.
\end{itemize}

\section{Results}
% \subsection{Performance analysis}
% \begin{itemize}
%     \item In Parla, for a given problem size, overall performance is dependent on the number of task created. I think the optimal performance depends on the balance between the task creation overhead and ability to overlap execution in tasks (i.e., overlap between data movement and computations)
%     \item Parla runtime grows with the number of tasks in task space for smaller problem sizes. The above is expected, since the overhead of the task creation and scheduling increases with the number of tasks. 
% \end{itemize}

\subsection{Parla, shared memory, and distributed memory comparison}
This  section presents the key result, which compares the Para shared memory multi-GPU implementation with hand-coded, python shared memory multi-GPU and python distributed memory TSQR implementations. Figure \ref{fig:parla_overall} presents the overall TSQR runtime while the cost breakdown for the each case presented in figure \ref{fig:parla_breakdown}. We keep the number of columns in the matrix fixed at 100 columns for all the Parla comparisons. For the Parla comparisons, the QR decomposition is done as follows.
\begin{itemize}
    \item Partitioned block qr : performed in GPU $[Q_p,R_p]=qr(A_p)$, only $R_p$ factor is return to the host. 
    \item Gather $R_p$ to $R$, and QR on cpu, $[Q2,R]=qr(R)$
    \item Scatter $Q2$ to $Q2_p$, matrix matrix multiplication on GPU. 
\end{itemize}

The performance difference between the 3 approaches can be summarized as follows. 
\begin{itemize}
    \item Why matmult task has different timings? 
    \begin{itemize}
        \item The final $Q$ matrix stays distributed for the MPI implementation, i.e.,  hence not concatenated for the final $Q$ matrix. 
        \item For Parla, the final Q global matrix is assembled, where the result of the matmult is directly written to the task portion of the global memory.
        \item In the thread-based parallelism case, the global Q matrix is assembled, but unlike the Parla, it is not directly written to the $Q$ global memory, hence has the cost of an additional data copy cost.  
    \end{itemize}
\end{itemize}

\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                ybar, 
                symbolic x coords={1K,10K,100K,1000K},
                xtick=data,
                width=12cm,
                height=4.5cm,
                grid=major,
                xlabel={number of rows $\rightarrow$}, ylabel={runtime (s) $\rightarrow$},
                %legend pos= north west,
                legend columns=3,
                legend style={at={(0.5,-0.35)},anchor=north},
                bar width=0.25cm,
            ]
            \addplot[bar shift =-0.3cm,fill=red!20] table[x={Rows},y={total_max}]{dat/frontera_mpi_gpu_v3.dat};
            \addplot[bar shift = 0.0cm,fill=blue!20] table[x={Rows},y={total}]{dat/frontera_sm_gpu.dat};
            \addplot[bar shift = 0.3cm,fill=orange!20] table[x={Rows},y={total}]{dat/frontera_parla_gpu_t4.dat};
            %\addplot[bar shift=0.2cm,fill=red!30] table[x={Rows},y={total}]{dat/frontera_parla_gpu_t8.dat};
            %\addplot[bar shift=0.4cm,fill=yellow!30] table[x={Rows},y={total}]{dat/frontera_parla_gpu_t16.dat};
            \legend{MPI+Cupy, Threads+Cupy, Parla+Cupy}
        \end{axis}
    \end{tikzpicture}
    \caption{Single node performance for TSQR algorithm implemented using Cupy where the parallelism was achieved by Python MPI, Python threading and Parla framework in Frontera GPU cluster. Each approach decompose the problem to 4 partitioned tasks, where the created tasks are assigned to its own GPU. For the above study, matrix column size was fixed at 500 columns.\label{fig:parla_overall}}
\end{figure}
\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                ybar stacked, 
                symbolic x coords={10K,100K,1000K},
                xtick=data,
                width=12cm,
                height=4.5cm,
                grid=major,
                xlabel={number of rows $\rightarrow$}, ylabel={runtime (s) $\rightarrow$},
                %legend pos= north west,
                legend columns=4,
                legend style={at={(0.5,-0.35)},anchor=north},
                bar width=0.25cm,
            ]
            \addplot[bar shift=-0.3cm,fill=red!30] table[x={Rows},y={qr1_max}]{dat/frontera_mpi_gpu_v3.dat};
            \addplot[bar shift=-0.3cm,fill=blue!30] table[x={Rows},y={qr2_max}]      {dat/frontera_mpi_gpu_v3.dat};
            \addplot[bar shift=-0.3cm,fill=orange!30] table[x={Rows},y={mm_max}]    {dat/frontera_mpi_gpu_v3.dat};
            \addplot[bar shift=-0.3cm,fill=green!20] table[x={Rows},y={mpi_comm_max}]{dat/frontera_mpi_gpu_v3.dat};
            \addplot[bar shift=-0.3cm,fill=black!20] table[x={Rows},y={overhead}]    {dat/frontera_mpi_gpu_v3.dat};
            \resetstackedplots
            \addplot[postaction={pattern=north east lines},bar shift=-0.0cm,fill=red!30] table[x={Rows},y={qr1}]       {dat/frontera_sm_gpu.dat};
            \addplot[postaction={pattern=north east lines},bar shift=-0.0cm,fill=blue!30] table[x={Rows},y={qr2}]      {dat/frontera_sm_gpu.dat};
            \addplot[postaction={pattern=north east lines},bar shift=-0.0cm,fill=orange!30] table[x={Rows},y={mm}]     {dat/frontera_sm_gpu.dat};
            \addplot[postaction={pattern=north east lines},bar shift=-0.0cm,fill=black!20] table[x={Rows},y={overhead}]{dat/frontera_sm_gpu.dat};
            \resetstackedplots
            \addplot[postaction={pattern=dots},bar shift=0.3cm,fill=red!30] table[x={Rows},y={qr1}]       {dat/frontera_parla_gpu_t4.dat};
            \addplot[postaction={pattern=dots},bar shift=0.3cm,fill=blue!30] table[x={Rows},y={qr2}]      {dat/frontera_parla_gpu_t4.dat};
            \addplot[postaction={pattern=dots},bar shift=0.3cm,fill=orange!30] table[x={Rows},y={mm}]     {dat/frontera_parla_gpu_t4.dat};
            \addplot[postaction={pattern=dots},bar shift=0.3cm,fill=black!20] table[x={Rows},y={overhead}]{dat/frontera_parla_gpu_t4.dat};
        \legend{qr1(MPI), qr2(MPI),mm(MPI),MPI\_comm,other(MPI),qr1(Threads),qr2(Threads), mm \& Q (Threads),other(threads),qr1(Parla),qr2(Parla), mm \& Q(Parla), other(Parla)}
        \end{axis}
    \end{tikzpicture}
    % \caption{Single node performance for TSQR algorithm implemented using Cupy where the parallelism achieved by, Python MPI, Python threading and Parla framework in Frontera GPU cluster. For this experiment matrix column size was fixed at 100 columns.}
    \caption{Overall cost breakdown for MPI + Cupy, Threads + Cupy and Parla using 4 blocks on 4 GPUs. \texttt{qr1} denotes the first QR decomposition on the GPU, \texttt{q2} denotes the second QR decomposition on the CPU, and \texttt{mm + Q} denotes the GPU matrix-matrix multiplication and the final $Q$ matrix computation. The timing difference for \texttt{mm \& Q}, mainly because, for MPI Q matrix stays distributed hence not gathered, for Parla and thread-based cases, the final $Q$ is assembled, but thread-based case uses one additional data copy in comparison to Parla. 
    \label{fig:parla_breakdown} }
\end{figure}


\subsection{Multi-node TSQR performance comparison}
This section presents, the weak and strong scaling results for distributed memory TSQR factorization. In the following experiments, the distributed memory parallelism is handled by MPI, and the shared memory based multi-GPU parallelism is achieved through parla. In MPI+Parla hybrid parallelism, MPI processes is launched for each node, with four threads per process. In pure MPI + Cupy based distributed parallelism case, four MPI tasks are launched for each node. 

\textbf{MPI gather operation and the second QR factorization}: In TSQR, the $R1r$ factors need to be gathered from the processors to perform the second QR factorization. The above operation has the complexity of $\mathbb{O}(pn^2\log(p))$. The input matrix size for the second QR factorization is $pn\times n$. Therefore the gather and the second QR factorization grows with $p$ for fixed $n$.

\textbf{Scaling plots}:
For the scaling plots (see Figures \ref{fig:parla_ws_n100}, \ref{fig:parla_ws_n1000}, \ref{fig:parla_ss_n100}, and \ref{fig:parla_ss_n1000}) for the ease of comparison overall cost breakdown follows the Parla tasks. 

\textbf{Results discussion}:

\begin{itemize}
    \item \textbf{MPI +Parla} : For each node 1 MPI task is launched with \texttt{OMP\_NUM\_THREADS}=16, and each process will create 4 tasks, 1 for each GPU on the node. 
    \item \textbf{MPI +Cupy} : For each node 4 MPI tasks are launched with \texttt{OMP\_NUM\_THREADS}=4, and each process will utilize its corresponding GPU assigned. 
\end{itemize}

\begin{itemize}
    \item Why qr2 has bad scaling ? 
    \begin{itemize}
        \item The input matrix size for the qr2 is $pn\times n$, where $p$ is the number of MPI tasks, and $n$ is the number of the columns. MPI + Parla has a smaller qr2 decomposition with more OMP threads, while in MPI + Parla case will have a bigger qr2 factorization with a smaller number of OMP threads. The above is the main reason for bad strong scalability for n=1000, MPI + Cupy case. 
    \end{itemize}
    \item \texttt{MM \& Q} computation 
    \begin{itemize}
        \item As mentioned in MPI, there is no global Q assembly between distributed memory processes. Therefore, for MPI + Cupy case it is only the matrix-matrix multiplication while in MPI + Parla case, the global $Q$ matrix is not assembled but the shared memory $Q$ matrix is concatenated with a copy operation. 
    \end{itemize}
    \item Why \texttt{qr1} is expensive in MPI + Parla case ? 
    \begin{itemize}
        \item Distributed memory matrix partitioning or matrix creation is not included in the overall time. 
        \item For MPI + Cupy case, once the execution begins in the local partition there is no additional partitioning step involved. 
        \item For MPI + Parala case, once the execution begins there is an additional shared memory block wise matrix partitioning cost associated with the qr1 task. 
    \end{itemize}
\end{itemize}

For the scaling plots the maximum time across all the MPI processors is used. 

\begin{itemize}
    \item Figure \ref{fig:parla_ws_n100} : Weak scaling results keeping number of columns fixed at 100.
    \item Figure \ref{fig:parla_ws_n1000} : Weak scaling results keeping number of columns fixed at 1000.
    \item Figure \ref{fig:parla_ss_n100} : Strong scaling results for fixed matrix size of $1.6M\times 100$.
    \item Figure \ref{fig:parla_ss_n1000} : Strong scaling results for fixed matrix size of $1.6M\times 1000$.
\end{itemize}

\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                ybar stacked, 
                symbolic x coords={1,2,4,8,16},
                xtick=data,
                width=12cm,
                height=4.5cm,
                grid=major,
                xlabel={number of nodes $\rightarrow$}, ylabel={runtime (s) $\rightarrow$},
                %legend pos= north west,
                legend columns=3,
                legend style={at={(0.5,-0.35)},anchor=north},
                bar width=0.3cm,
            ]
            \addplot[bar shift=0.0cm,fill=red!20]    table[x={nodes},y={qr1_max}]        {dat/frontera_parla_c100_ws_gz_100K.dat};
            \addplot[bar shift=0.0cm,fill=yellow!20] table[x={nodes},y={qr2_max}]        {dat/frontera_parla_c100_ws_gz_100K.dat};
            \addplot[bar shift=0.0cm,fill=cyan!20]   table[x={nodes},y={mm_max}]         {dat/frontera_parla_c100_ws_gz_100K.dat};
            \addplot[bar shift=0.0cm,fill=green!20]  table[x={nodes},y={mpi_comm_max}]   {dat/frontera_parla_c100_ws_gz_100K.dat};
            \addplot[bar shift=0.0cm,fill=black!20]  table[x={nodes},y={overhead}]       {dat/frontera_parla_c100_ws_gz_100K.dat};

            \resetstackedplotsOne
            \addplot[postaction={pattern=north east lines}, bar shift=-0.4cm,fill=red!20]    table[x={nodes},y={qr1_max}]        {dat/frontera_mpi_c100_ws_gz_100K.dat};
            \addplot[postaction={pattern=north east lines}, bar shift=-0.4cm,fill=yellow!20] table[x={nodes},y={qr2_max}]        {dat/frontera_mpi_c100_ws_gz_100K.dat};
            \addplot[postaction={pattern=north east lines}, bar shift=-0.4cm,fill=cyan!20]   table[x={nodes},y={mm_max}]         {dat/frontera_mpi_c100_ws_gz_100K.dat};
            \addplot[postaction={pattern=north east lines}, bar shift=-0.4cm,fill=green!20]  table[x={nodes},y={mpi_comm_max}]   {dat/frontera_mpi_c100_ws_gz_100K.dat};
            \addplot[postaction={pattern=north east lines}, bar shift=-0.4cm,fill=black!20]  table[x={nodes},y={overhead}]       {dat/frontera_mpi_c100_ws_gz_100K.dat};

            \resetstackedplotsOne
            \addplot[postaction={pattern=dots}, bar shift=0.4cm,fill=red!20]    table[x={nodes},y={qr1_max}]        {dat/frontera_parla_c100_ws_100K_16.dat};
            \addplot[postaction={pattern=dots}, bar shift=0.4cm,fill=yellow!20] table[x={nodes},y={qr2_max}]        {dat/frontera_parla_c100_ws_100K_16.dat};
            \addplot[postaction={pattern=dots}, bar shift=0.4cm,fill=cyan!20]   table[x={nodes},y={mm_max}]         {dat/frontera_parla_c100_ws_100K_16.dat};
            \addplot[postaction={pattern=dots}, bar shift=0.4cm,fill=green!20]  table[x={nodes},y={mpi_comm_max}]   {dat/frontera_parla_c100_ws_100K_16.dat};
            \addplot[postaction={pattern=dots}, bar shift=0.4cm,fill=black!20]  table[x={nodes},y={overhead}]       {dat/frontera_parla_c100_ws_100K_16.dat};

            \legend{qr1(MPI+Cupy), qr2(MPI+Cupy), MM(MPI+Cupy), MPI\_Comm(MPI+Cupy), other (MPI+Cupy), qr1(MPI+Parla), qr2(MPI+Parla), MM(MPI+Parla), MPI\_Comm (MPI+Parla), other(MPI+Parla), qr1(MPI+Parla 16t), qr2(MPI+Parla 16t), MM(MPI+Parla 16t), MPI\_Comm (MPI+Parla 16t), other(MPI+Parla 16t),}
        \end{axis}
    \end{tikzpicture}
    \caption{The figure presents the weak scalability results for 100K rows per node and the number of columns fixed at 100 for MPI + Parla based hybrid parallelism and MPI + Cupy based implementations across 16 nodes (64 GPUs) in the Frontera GPU cluster. Note that for Parla+Cupy case, tasks are dynamically created and scheduled at the runtime, where in MPI+Cupy tasks creation and execution statically coded in the implementation.  \label{fig:parla_ws_n100}  }
\end{figure}
\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            ybar stacked, 
            symbolic x coords={1,2,4,8,16},
            xtick=data,
            width=12cm,
            height=4.5cm,
            grid=major,
            xlabel={number of nodes $\rightarrow$}, ylabel={runtime (s) $\rightarrow$},
            %legend pos= north west,
            legend columns=3,
            legend style={at={(0.5,-0.35)},anchor=north},
            bar width=0.3cm,
        ]
        \addplot[bar shift=0.0cm,fill=red!20]    table[x={nodes},y={qr1_max}]        {dat/frontera_parla_c1000_ws_gz_100K.dat};
        \addplot[bar shift=0.0cm,fill=yellow!20] table[x={nodes},y={qr2_max}]        {dat/frontera_parla_c1000_ws_gz_100K.dat};
        \addplot[bar shift=0.0cm,fill=cyan!20]   table[x={nodes},y={mm_max}]         {dat/frontera_parla_c1000_ws_gz_100K.dat};
        \addplot[bar shift=0.0cm,fill=green!20]  table[x={nodes},y={mpi_comm_max}]   {dat/frontera_parla_c1000_ws_gz_100K.dat};
        \addplot[bar shift=0.0cm,fill=black!20]  table[x={nodes},y={overhead}]       {dat/frontera_parla_c1000_ws_gz_100K.dat};

        \resetstackedplotsOne
        \addplot[postaction={pattern=north east lines}, bar shift=-0.4cm,fill=red!20]    table[x={nodes},y={qr1_max}]        {dat/frontera_mpi_c1000_ws_gz_100K.dat};
        \addplot[postaction={pattern=north east lines}, bar shift=-0.4cm,fill=yellow!20] table[x={nodes},y={qr2_max}]        {dat/frontera_mpi_c1000_ws_gz_100K.dat};
        \addplot[postaction={pattern=north east lines}, bar shift=-0.4cm,fill=cyan!20]   table[x={nodes},y={mm_max}]         {dat/frontera_mpi_c1000_ws_gz_100K.dat};
        \addplot[postaction={pattern=north east lines}, bar shift=-0.4cm,fill=green!20]  table[x={nodes},y={mpi_comm_max}]   {dat/frontera_mpi_c1000_ws_gz_100K.dat};
        \addplot[postaction={pattern=north east lines}, bar shift=-0.4cm,fill=black!20]  table[x={nodes},y={overhead}]       {dat/frontera_mpi_c1000_ws_gz_100K.dat};

        \resetstackedplotsOne
        \addplot[postaction={pattern=dots}, bar shift=0.4cm,fill=red!20]    table[x={nodes},y={qr1_max}]        {dat/frontera_parla_c1000_ws_100K_16.dat};
        \addplot[postaction={pattern=dots}, bar shift=0.4cm,fill=yellow!20] table[x={nodes},y={qr2_max}]        {dat/frontera_parla_c1000_ws_100K_16.dat};
        \addplot[postaction={pattern=dots}, bar shift=0.4cm,fill=cyan!20]   table[x={nodes},y={mm_max}]         {dat/frontera_parla_c1000_ws_100K_16.dat};
        \addplot[postaction={pattern=dots}, bar shift=0.4cm,fill=green!20]  table[x={nodes},y={mpi_comm_max}]   {dat/frontera_parla_c1000_ws_100K_16.dat};
        \addplot[postaction={pattern=dots}, bar shift=0.4cm,fill=black!20]  table[x={nodes},y={overhead}]       {dat/frontera_parla_c1000_ws_100K_16.dat};

        \legend{qr1(MPI+Cupy), qr2(MPI+Cupy), MM(MPI+Cupy), MPI\_Comm(MPI+Cupy), other (MPI+Cupy), qr1(MPI+Parla), qr2(MPI+Parla), MM(MPI+Parla), MPI\_Comm (MPI+Parla), other(MPI+Parla), qr1(MPI+Parla 16t), qr2(MPI+Parla 16t), MM(MPI+Parla 16t), MPI\_Comm (MPI+Parla 16t), other(MPI+Parla 16t),}
    \end{axis}
    \end{tikzpicture}
    \caption{The figure presents the weak scalability results for 100K rows per node and the number of columns fixed at 1000 for MPI + Parla based hybrid parallelism and MPI + Cupy based implementations across 16 nodes (64 GPUs) in the Frontera GPU cluster. Note that for Parla+Cupy case, tasks are dynamically created and scheduled at the runtime, where in MPI+Cupy tasks creation and execution statically coded in the implementation.\label{fig:parla_ws_n1000} }
\end{figure}
\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            ybar stacked, 
            symbolic x coords={1,2,4,8,16},
            xtick=data,
            width=12cm,
            height=4.5cm,
            grid=major,
            xlabel={number of nodes $\rightarrow$}, ylabel={runtime (s) $\rightarrow$},
            %legend pos= north west,
            legend columns=3,
            legend style={at={(0.5,-0.35)},anchor=north},
            bar width=0.3cm,
        ]
        \addplot[bar shift=0.0cm,fill=red!20]    table[x={nodes},y={qr1_max}]        {dat/frontera_parla_c100_ss_1600K_4.dat};
        \addplot[bar shift=0.0cm,fill=yellow!20] table[x={nodes},y={qr2_max}]        {dat/frontera_parla_c100_ss_1600K_4.dat};
        \addplot[bar shift=0.0cm,fill=cyan!20]   table[x={nodes},y={mm_max}]         {dat/frontera_parla_c100_ss_1600K_4.dat};
        \addplot[bar shift=0.0cm,fill=green!20]  table[x={nodes},y={mpi_comm_max}]   {dat/frontera_parla_c100_ss_1600K_4.dat};
        \addplot[bar shift=0.0cm,fill=black!20]  table[x={nodes},y={overhead}]       {dat/frontera_parla_c100_ss_1600K_4.dat};

        \resetstackedplotsOne
        \addplot[postaction={pattern=north east lines}, bar shift=-0.4cm,fill=red!20]    table[x={nodes},y={qr1_max}]        {dat/frontera_mpi_c100_ss_1600K.dat};
        \addplot[postaction={pattern=north east lines}, bar shift=-0.4cm,fill=yellow!20] table[x={nodes},y={qr2_max}]        {dat/frontera_mpi_c100_ss_1600K.dat};
        \addplot[postaction={pattern=north east lines}, bar shift=-0.4cm,fill=cyan!20]   table[x={nodes},y={mm_max}]         {dat/frontera_mpi_c100_ss_1600K.dat};
        \addplot[postaction={pattern=north east lines}, bar shift=-0.4cm,fill=green!20]  table[x={nodes},y={mpi_comm_max}]   {dat/frontera_mpi_c100_ss_1600K.dat};
        \addplot[postaction={pattern=north east lines}, bar shift=-0.4cm,fill=black!20]  table[x={nodes},y={overhead}]       {dat/frontera_mpi_c100_ss_1600K.dat};

        \resetstackedplotsOne
        \addplot[postaction={pattern=dots}, bar shift=0.4cm,fill=red!20]    table[x={nodes},y={qr1_max}]        {dat/frontera_parla_c100_ss_1600K_16.dat};
        \addplot[postaction={pattern=dots}, bar shift=0.4cm,fill=yellow!20] table[x={nodes},y={qr2_max}]        {dat/frontera_parla_c100_ss_1600K_16.dat};
        \addplot[postaction={pattern=dots}, bar shift=0.4cm,fill=cyan!20]   table[x={nodes},y={mm_max}]         {dat/frontera_parla_c100_ss_1600K_16.dat};
        \addplot[postaction={pattern=dots}, bar shift=0.4cm,fill=green!20]  table[x={nodes},y={mpi_comm_max}]   {dat/frontera_parla_c100_ss_1600K_16.dat};
        \addplot[postaction={pattern=dots}, bar shift=0.4cm,fill=black!20]  table[x={nodes},y={overhead}]       {dat/frontera_parla_c100_ss_1600K_16.dat};

        \legend{qr1(MPI+Cupy), qr2(MPI+Cupy), MM(MPI+Cupy), MPI\_Comm(MPI+Cupy), other (MPI+Cupy), qr1(MPI+Parla), qr2(MPI+Parla), MM(MPI+Parla), MPI\_Comm (MPI+Parla), other(MPI+Parla), qr1(MPI+Parla 16t), qr2(MPI+Parla 16t), MM(MPI+Parla 16t), MPI\_Comm (MPI+Parla 16t), other(MPI+Parla 16t),}
    \end{axis}
    \end{tikzpicture}
    \caption{The figure presents the strong scalability results for a fixed matrix size of $1.6M \times 100$ for MPI + Parla based hybrid parallelism and MPI + Cupy based implementations across 16 nodes (64 GPUs) in the Frontera GPU cluster. Note that for Parla+Cupy case, tasks are dynamically created and scheduled at the runtime, where in MPI+Cupy tasks creation and execution statically coded in the implementation. \label{fig:parla_ss_n100}
    }
\end{figure}
\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            ybar stacked, 
            symbolic x coords={1,2,4,8,16},
            xtick=data,
            width=12cm,
            height=4.5cm,
            grid=major,
            xlabel={number of nodes $\rightarrow$}, ylabel={runtime (s) $\rightarrow$},
            %legend pos= north west,
            legend columns=3,
            legend style={at={(0.5,-0.35)},anchor=north},
            bar width=0.3cm,
        ]
        \addplot[bar shift=0.0cm,fill=red!20]    table[x={nodes},y={qr1_max}]        {dat/frontera_parla_c1000_ss_1600K_4.dat};
        \addplot[bar shift=0.0cm,fill=yellow!20] table[x={nodes},y={qr2_max}]        {dat/frontera_parla_c1000_ss_1600K_4.dat};
        \addplot[bar shift=0.0cm,fill=cyan!20]   table[x={nodes},y={mm_max}]         {dat/frontera_parla_c1000_ss_1600K_4.dat};
        \addplot[bar shift=0.0cm,fill=green!20]  table[x={nodes},y={mpi_comm_max}]   {dat/frontera_parla_c1000_ss_1600K_4.dat};
        \addplot[bar shift=0.0cm,fill=black!20]  table[x={nodes},y={overhead}]       {dat/frontera_parla_c1000_ss_1600K_4.dat};

        \resetstackedplotsOne
        \addplot[postaction={pattern=north east lines}, bar shift=-0.4cm,fill=red!20]    table[x={nodes},y={qr1_max}]        {dat/frontera_mpi_c1000_ss_1600K.dat};
        \addplot[postaction={pattern=north east lines}, bar shift=-0.4cm,fill=yellow!20] table[x={nodes},y={qr2_max}]        {dat/frontera_mpi_c1000_ss_1600K.dat};
        \addplot[postaction={pattern=north east lines}, bar shift=-0.4cm,fill=cyan!20]   table[x={nodes},y={mm_max}]         {dat/frontera_mpi_c1000_ss_1600K.dat};
        \addplot[postaction={pattern=north east lines}, bar shift=-0.4cm,fill=green!20]  table[x={nodes},y={mpi_comm_max}]   {dat/frontera_mpi_c1000_ss_1600K.dat};
        \addplot[postaction={pattern=north east lines}, bar shift=-0.4cm,fill=black!20]  table[x={nodes},y={overhead}]       {dat/frontera_mpi_c1000_ss_1600K.dat};

        \resetstackedplotsOne
        \addplot[postaction={pattern=dots}, bar shift=0.4cm,fill=red!20]    table[x={nodes},y={qr1_max}]        {dat/frontera_parla_c1000_ss_1600K_16.dat};
        \addplot[postaction={pattern=dots}, bar shift=0.4cm,fill=yellow!20] table[x={nodes},y={qr2_max}]        {dat/frontera_parla_c1000_ss_1600K_16.dat};
        \addplot[postaction={pattern=dots}, bar shift=0.4cm,fill=cyan!20]   table[x={nodes},y={mm_max}]         {dat/frontera_parla_c1000_ss_1600K_16.dat};
        \addplot[postaction={pattern=dots}, bar shift=0.4cm,fill=green!20]  table[x={nodes},y={mpi_comm_max}]   {dat/frontera_parla_c1000_ss_1600K_16.dat};
        \addplot[postaction={pattern=dots}, bar shift=0.4cm,fill=black!20]  table[x={nodes},y={overhead}]       {dat/frontera_parla_c1000_ss_1600K_16.dat};

        \legend{qr1(MPI+Cupy), qr2(MPI+Cupy), MM(MPI+Cupy), MPI\_Comm(MPI+Cupy), other (MPI+Cupy), qr1(MPI+Parla), qr2(MPI+Parla), MM(MPI+Parla), MPI\_Comm (MPI+Parla), other(MPI+Parla), qr1(MPI+Parla 16t), qr2(MPI+Parla 16t), MM(MPI+Parla 16t), MPI\_Comm (MPI+Parla 16t), other(MPI+Parla 16t),}
    \end{axis}
    \end{tikzpicture}
    \caption{The figure presents the strong scalability results for a fixed matrix size of $1.6M \times 1000$ for MPI + Parla based hybrid parallelism and MPI + Cupy based implementations across 16 nodes (64 GPUs) in the Frontera GPU cluster.Note that for Parla+Cupy case, tasks are dynamically created and scheduled at the runtime, where in MPI+Cupy tasks creation and execution statically coded in the implementation. \label{fig:parla_ss_n1000}
    }
\end{figure}


\subsection{Single GPU performance analysis}
This section presents a single GPU performance analysis to perform the QR decomposition of varying matrix sizes. 
\begin{figure}[!tbhp]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                symbolic x coords={1K,10K,100K,1000K},
                xtick=data,
                width=12cm,
                height=5cm,
                grid=major,
                xlabel={number of rows $\rightarrow$}, ylabel={GB/s $\rightarrow$ },
                %legend pos=outer north east,legend columns=1,
                legend columns=3,
                legend style={at={(0.5,-0.3)},anchor=north},
            ]
            \addplot[mark=*,red] table[x={NROWS},y={H2D_BW(GB/sec)}]{dat/rtx5000_qr_n10.dat};
            \addplot[mark=square,blue] table[x={NROWS},y={H2D_BW(GB/sec)}]{dat/rtx5000_qr_n100.dat};
            \addplot[mark=triangle,green] table[x={NROWS},y={H2D_BW(GB/sec)}]{dat/rtx5000_qr_n1000.dat};
            \addplot[mark=*,red,dashed] table[x={NROWS},y={D2H_BW(GB/sec)}]{dat/rtx5000_qr_n10.dat};
            \addplot[mark=square,blue,dashed] table[x={NROWS},y={D2H_BW(GB/sec)}]{dat/rtx5000_qr_n100.dat};
            \addplot[mark=triangle,green,dashed] table[x={NROWS},y={D2H_BW(GB/sec)}]{dat/rtx5000_qr_n1000.dat};
            
            \legend{H2D (n=10),H2D (n=100), H2D (n=1000),D2H (n=10),D2H (n=100), D2H (n=1000)} 
        \end{axis}
    \end{tikzpicture}
    \caption{Empirical data transfer bandwidth for cupy transfer of $2d$ numpy arrays. For a matrix $A$ with $m$ rows and $n$ columns, host to device (H2D) transfer has the complexity of $\mathbb{O}(mn)$. The device to host (D2H) transfers copy two cupy arrays (computed $Q,R$ factors) to numpy arrays with the complexity of $\mathbb{O}(mn + n^2)$.}
\end{figure}
\begin{figure}[!bhp]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                symbolic x coords={1K,10K,100K,1000K},
                xtick=data,
                width=12cm,
                height=5cm,
                grid=major,
                xlabel={number of rows $\rightarrow$ }, ylabel={GFlops/s $\rightarrow$},
                %legend pos=outer north east,legend columns=1,
                legend columns=3,
                legend style={at={(0.5,-0.3)},anchor=north},
            ]
            \addplot[mark=*,red] table[x={NROWS},y={QR_FLOPS(GFlops/sec)}]{dat/rtx5000_qr_n10.dat};
            \addplot[mark=square,blue] table[x={NROWS},y={QR_FLOPS(GFlops/sec)}]{dat/rtx5000_qr_n100.dat};
            \addplot[mark=triangle,green] table[x={NROWS},y={QR_FLOPS(GFlops/sec)}]{dat/rtx5000_qr_n1000.dat};
            \legend{n=10,n=100,n=1000} 
        \end{axis}
    \end{tikzpicture}
    \caption{GFlops/s attained by \texttt{cupy.linalg.qr} where GFlops/s is computed based on $\mathbb{O}(2mn^2 - \frac{2}{3}n^3)$ floating point operations for different matrix sizes, on a single RTX5000 GPU node, where $n$ denotes the number of columns.}
\end{figure}
\begin{figure}[!tbhp]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                symbolic x coords={1K,10K,100K,1000K},
                xtick=data,
                width=12cm,
                height=6cm,
                grid=major,
                xlabel={number of rows $\rightarrow$ }, ylabel={GFlops/s $\rightarrow$},
                %legend pos=outer north east,legend columns=1,
                legend columns=3,
                legend style={at={(0.5,-0.25)},anchor=north},
            ]
            \addplot[mark=*,red] table[x={NROWS},y={QR_FLOPS(GFlops/sec)}]{dat/frontera_rtx_cpu_qr_n10.dat};
            \addplot[mark=square,blue] table[x={NROWS},y={QR_FLOPS(GFlops/sec)}]{dat/frontera_rtx_cpu_qr_n100.dat};
            \addplot[mark=triangle,green] table[x={NROWS},y={QR_FLOPS(GFlops/sec)}]{dat/frontera_rtx_cpu_qr_n1000.dat};
            \legend{n=10,n=100,n=1000} 
        \end{axis}
    \end{tikzpicture}
    \caption{GFlops/s attained by \texttt{numpy.linalg.qr} where GFlops/s is computed based on $\mathbb{O}(2mn^2 - \frac{2}{3}n^3)$ floating point operations for different matrix sizes, on frontera RTX partition CPU node, where $n$ denotes the number of columns.}
\end{figure}
\begin{figure}[!bhp]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                symbolic x coords={1K,10K,100K,1000K},
                xtick=data,
                width=12cm,
                height=6cm,
                grid=major,
                xlabel={number of rows $\rightarrow$ }, ylabel={GFlops/s $\rightarrow$},
                %legend pos=outer north east,legend columns=1,
                legend columns=3,
                legend style={at={(0.5,-0.25)},anchor=north},
            ]
            \addplot[mark=*,red] table[x={NROWS},y={MM_FLOPS(GFlops/sec)}]{dat/rtx5000_mm_n10.dat};
            \addplot[mark=square,blue] table[x={NROWS},y={MM_FLOPS(GFlops/sec)}]{dat/rtx5000_mm_n100.dat};
            \addplot[mark=triangle,green] table[x={NROWS},y={MM_FLOPS(GFlops/sec)}]{dat/rtx5000_mm_n1000.dat};
            \legend{n=10,n=100,n=1000} 
        \end{axis}
    \end{tikzpicture}
    \caption{GFlops/s  attained by \texttt{cupy.matmult} where  GFlops/s is computed based on $\mathbb{O}(mn(2n-1))$ floating point operations for different matrix sizes, on a single RTX5000 GPU node, where $n$ denotes the number of columns.}
\end{figure}


\subsection{Performance model for shared memory multi-gpu TSQR}
Let $B_{H2D}$, $B_{D2H}$ be the bandwidth for the host to device and device to host data transfers, respectively. Let $QR_{gpu}(m,n)$ and $QR_{cpu}(m,n)$ be the flops/s for the cupy QR and numpy QR decomposition for a given number of rows $m$ and columns $n$. Let $MM_{gpu}(m,n)$ be the flops/s for GPU execution of general matrix-matrix multiplication. Let p be the number of threads (1 per GPU) in the shared memory parallel multi-GPU execution. Then the overall time can be approximated by, 
\begin{align}
    T(m,n,p) &= T(\frac{m}{p}, n)_{qr1,GPU}  + T(np,n)_{qr2,CPU} + T(\frac{m}{p},m)_{mm,GPU} \label{eq:qr_sm}
\end{align} where we can write, 
\begin{equation}
    T(\frac{m}{p}, n)_{qr1,GPU} = \frac{mn}{pB_{H2D}} + \frac{\frac{2mn^2}{p} -\frac{2}{3}n^3}{QR_{gpu}} + \frac{n^2}{B_{D2H}} \label{eq:qr1_sm}
\end{equation}
\begin{equation}
    T(np,n)_{qr2,CPU} = \frac{2pn^3 -\frac{2}{3}n^3}{QR_{cpu}} \label{eq:qr2_sm}
\end{equation}
\begin{equation}
    T(\frac{m}{p},m)_{mm,GPU} = \frac{n^2}{B_{H2D}} + \frac{\frac{mn}{p}(2n-1)}{MM_{gpu}} + \frac{mn}{pH_{D2H}} \label{eq:mm_sm}
\end{equation}

Tables \ref{tb:qr1} and \ref{tb:mm} presents the predicted time for partition QR and matrix-matrix multiplication respectively. For partitioned QR (see Table \ref{tb:qr1}), the difference between presented and reported runtime is almost constant because collecting of $R_p$ matrix from mapped execution is fixed because the size of $R_p$ is independent of the number of rows in the input matrix. 

In contrast with partitioned QR, the final step of the TSQR algorithm is performing a matrix-matrix multiplication to get the final $Q$ matrix. The collection overhead of final $Q_p=Q1_pQ2_p$ matrix from mapped thread pool execution grows with the number of rows in the input matrix, since $Q_p$ is $\frac{m}{p} \times n$ matrix. 


\begin{table}[!tbhp]
    \begin{center}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{ |c|c|c|c|c|c|c|c|c| } 
         \hline
         Rows & Cols & H2D(GB/s) &  D2H(GB/s)  &  QR(Flops/s) &  t1 (s) (runtime) & t2(s) (predicted) & t1-t2 & (t1-t2)/t1\\
         \hline
            1000	& 100 & 2.13 & 	1.12  &	2.35	& 0.014914022	& 0.001948	& 0.012966022 & 0.869384662 \\
            10000	& 100 & 4.77 &  3.95  &	12.23	& 0.017084167	& 0.00443	& 0.012654167 & 0.740695579 \\
            100000	& 100 & 4.71 &  6.08  &	22.21	& 0.045916252	& 0.026449	& 0.019467252 & 0.423973017 \\
            1000000	& 100 & 4.57 &  2.86  &	34.87	& 0.212147208	& 0.184173	& 0.027974208 & 0.131862249 \\
         \hline
        \end{tabular}}
    \end{center}
    \caption{Table presents the predicted QR1 runtime (see (\ref{eq:qr1_sm})) based on the empirically compute bandwidths and flops/s for matrix rows of $\frac{m}{p} \times n$, where $p$ denotes the number of GPUs used for the actual computation. The difference between the predicted and the reported runtime is almost a constant.\label{tb:qr1}}
\end{table}

\begin{table}[!tbhp]
    \begin{center}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{ |c|c|c|c|c|c|c|c|c| } 
         \hline
         Rows & Cols & H2D(GB/s) &  D2H(GB/s)  &  MM(Flops/s) &  t1 (s) (runtime) & t2(s) (predicted) & t1-t2 & (t1-t2)/t1\\
         \hline
            1000	& 100 & 2.29	& 2.40 & 	28.38   &	0.00404992	& 0.000185	& 0.00386492  & 0.954320085\\
            10000	& 100 & 4.85	& 4.97 & 	160.31  &	0.005135891	& 0.000358	& 0.004777891 & 0.930294471\\
            100000	& 100 & 4.98	& 6.66 &    209.65  &	0.011140349	& 0.002726	& 0.008414349 & 0.755303893\\
            1000000	& 100 & 4.21	& 2.91 & 	225.91  &	0.119347156	& 0.030051	& 0.089296156 & 0.748205144\\
         \hline
        \end{tabular}}
    \end{center}
    \caption{Table presents the predicted MM runtime (see (\ref{eq:mm_sm})) based on the empirically compute bandwidths and flops/s for matrix rows of $\frac{m}{p} \times n$, where $p$ denotes the number of GPUs used for the actual computation. The difference between the predicted and the reported runtime grows with $\frac{m}{p}$. \label{tb:mm}}
\end{table}



\bibliographystyle{plain}
\bibliography{parla_qr}



\end{document}





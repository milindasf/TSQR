\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{pgfplots}
\usetikzlibrary{patterns}

\makeatletter
\newcommand\resetstackedplots{
\makeatletter
\pgfplots@stacked@isfirstplottrue
\makeatother
\addplot [forget plot,draw=none] coordinates{(1K,0) (10K,0) (100K,0) (1000K,0)};
}
\makeatother

\makeatletter
\newcommand\resetstackedplotsOne{
\makeatletter
\pgfplots@stacked@isfirstplottrue
\makeatother
\addplot [forget plot,draw=none] coordinates{(1,0) (2,0) (4,0) (8,0) (16,0)};
}
\makeatother


\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\newcommand{\R}{\mathbb{R}}

\title{Parla Performance Study}
%\author{Milinda Fernando and Geroge Biros}

\begin{document}
\maketitle

\section{Benchmark} 
We use the tall skinny QR (TSQR) decomposition algorithm proposed in \cite{benson2013direct,demmel2008communication}. For a given matrix $A\in \R^{m\times n}$ we assume $m>>n$. The parallel TSQR algorithm can be summarized as follows. 

\begin{algorithm}
    \caption{TSQR decomposition}
    \begin{algorithmic} 
    \REQUIRE $A \in \R^{m\times n}, m>>n$, $p$ parallel tasks
    \ENSURE $A=QR$ for $Q\in \R^{m\times n }$, $Q^TQ=I$ and $R$ is upper triangular.
    \STATE $Ar \leftarrow Partition(A,p)$
    \STATE $Q1r,R1r \leftarrow QR(Ar)$
    \STATE $R1 \leftarrow Gather(R1r,p)$
    \STATE $Q2,R \leftarrow Qr(R1)$
    \STATE $Q2r \leftarrow Partition(Q2r)$ \COMMENT{Same parition bounds as $A$}
    \STATE $Q\leftarrow Q1r \times Q2r$
    \RETURN $Q,R$
    \end{algorithmic}
\end{algorithm}

\section{Implementation}
The following 3 implementations are considered in this performance evaluation. 
\begin{itemize}
    \item Parla QR + Cupy
    \item Python thread pool execution + Cupy
    \item Python MPI + Cupy
\end{itemize}

\section{Experimental setup}
\begin{itemize}
    \item The experiments are conducted in Frontera, GPU cluster, where a single node consists of Two Intel Xeon E5-2620 v4 (“Broadwell”) with four NVIDIA Quadro RTX5000 GPUs.
\end{itemize}

\section{Results}
% \subsection{Performance analysis}
% \begin{itemize}
%     \item In Parla, for a given problem size, overall performance is dependent on the number of task created. I think the optimal performance depends on the balance between the task creation overhead and ability to overlap execution in tasks (i.e., overlap between data movement and computations)
%     \item Parla runtime grows with the number of tasks in task space for smaller problem sizes. The above is expected, since the overhead of the task creation and scheduling increases with the number of tasks. 
% \end{itemize}


\subsection{Single GPU performance analysis}
This section presents a single GPU performance analysis to perform the QR decomposition of varying matrix sizes. 
\begin{figure}[!tbhp]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                symbolic x coords={1K,10K,100K,1000K},
                xtick=data,
                width=12cm,
                height=5cm,
                grid=major,
                xlabel={number of rows $\rightarrow$}, ylabel={GB/s $\rightarrow$ },
                %legend pos=outer north east,legend columns=1,
                legend columns=3,
                legend style={at={(0.5,-0.3)},anchor=north},
            ]
            \addplot[mark=*,red] table[x={NROWS},y={H2D_BW(GB/sec)}]{dat/rtx5000_qr_n10.dat};
            \addplot[mark=square,blue] table[x={NROWS},y={H2D_BW(GB/sec)}]{dat/rtx5000_qr_n100.dat};
            \addplot[mark=triangle,green] table[x={NROWS},y={H2D_BW(GB/sec)}]{dat/rtx5000_qr_n1000.dat};
            \addplot[mark=*,red,dashed] table[x={NROWS},y={D2H_BW(GB/sec)}]{dat/rtx5000_qr_n10.dat};
            \addplot[mark=square,blue,dashed] table[x={NROWS},y={D2H_BW(GB/sec)}]{dat/rtx5000_qr_n100.dat};
            \addplot[mark=triangle,green,dashed] table[x={NROWS},y={D2H_BW(GB/sec)}]{dat/rtx5000_qr_n1000.dat};
            
            \legend{H2D (n=10),H2D (n=100), H2D (n=1000),D2H (n=10),D2H (n=100), D2H (n=1000)} 
        \end{axis}
    \end{tikzpicture}
    \caption{Empirical data transfer bandwidth for cupy transfer of $2d$ numpy arrays. For a matrix $A$ with $m$ rows and $n$ columns, host to device (H2D) transfer has the complexity of $\mathbb{O}(mn)$. The device to host (D2H) transfers copy two cupy arrays (computed $Q,R$ factors) to numpy arrays with the complexity of $\mathbb{O}(mn + n^2)$.}
\end{figure}
\begin{figure}[!bhp]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                symbolic x coords={1K,10K,100K,1000K},
                xtick=data,
                width=12cm,
                height=5cm,
                grid=major,
                xlabel={number of rows $\rightarrow$ }, ylabel={GFlops/s $\rightarrow$},
                %legend pos=outer north east,legend columns=1,
                legend columns=3,
                legend style={at={(0.5,-0.3)},anchor=north},
            ]
            \addplot[mark=*,red] table[x={NROWS},y={QR_FLOPS(GFlops/sec)}]{dat/rtx5000_qr_n10.dat};
            \addplot[mark=square,blue] table[x={NROWS},y={QR_FLOPS(GFlops/sec)}]{dat/rtx5000_qr_n100.dat};
            \addplot[mark=triangle,green] table[x={NROWS},y={QR_FLOPS(GFlops/sec)}]{dat/rtx5000_qr_n1000.dat};
            \legend{n=10,n=100,n=1000} 
        \end{axis}
    \end{tikzpicture}
    \caption{GFlops/s attained by \texttt{cupy.linalg.qr} where GFlops/s is computed based on $\mathbb{O}(2mn^2 - \frac{2}{3}n^3)$ floating point operations for different matrix sizes, on a single RTX5000 GPU node, where $n$ denotes the number of columns.}
\end{figure}
\begin{figure}[!tbhp]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                symbolic x coords={1K,10K,100K,1000K},
                xtick=data,
                width=12cm,
                height=6cm,
                grid=major,
                xlabel={number of rows $\rightarrow$ }, ylabel={GFlops/s $\rightarrow$},
                %legend pos=outer north east,legend columns=1,
                legend columns=3,
                legend style={at={(0.5,-0.25)},anchor=north},
            ]
            \addplot[mark=*,red] table[x={NROWS},y={QR_FLOPS(GFlops/sec)}]{dat/frontera_rtx_cpu_qr_n10.dat};
            \addplot[mark=square,blue] table[x={NROWS},y={QR_FLOPS(GFlops/sec)}]{dat/frontera_rtx_cpu_qr_n100.dat};
            \addplot[mark=triangle,green] table[x={NROWS},y={QR_FLOPS(GFlops/sec)}]{dat/frontera_rtx_cpu_qr_n1000.dat};
            \legend{n=10,n=100,n=1000} 
        \end{axis}
    \end{tikzpicture}
    \caption{GFlops/s attained by \texttt{numpy.linalg.qr} where GFlops/s is computed based on $\mathbb{O}(2mn^2 - \frac{2}{3}n^3)$ floating point operations for different matrix sizes, on frontera RTX partition CPU node, where $n$ denotes the number of columns.}
\end{figure}
\begin{figure}[!bhp]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                symbolic x coords={1K,10K,100K,1000K},
                xtick=data,
                width=12cm,
                height=6cm,
                grid=major,
                xlabel={number of rows $\rightarrow$ }, ylabel={GFlops/s $\rightarrow$},
                %legend pos=outer north east,legend columns=1,
                legend columns=3,
                legend style={at={(0.5,-0.25)},anchor=north},
            ]
            \addplot[mark=*,red] table[x={NROWS},y={MM_FLOPS(GFlops/sec)}]{dat/rtx5000_mm_n10.dat};
            \addplot[mark=square,blue] table[x={NROWS},y={MM_FLOPS(GFlops/sec)}]{dat/rtx5000_mm_n100.dat};
            \addplot[mark=triangle,green] table[x={NROWS},y={MM_FLOPS(GFlops/sec)}]{dat/rtx5000_mm_n1000.dat};
            \legend{n=10,n=100,n=1000} 
        \end{axis}
    \end{tikzpicture}
    \caption{GFlops/s  attained by \texttt{cupy.matmult} where  GFlops/s is computed based on $\mathbb{O}(mn(2n-1))$ floating point operations for different matrix sizes, on a single RTX5000 GPU node, where $n$ denotes the number of columns.}
\end{figure}

\subsection{Parla, shared memory, and distributed memory comparison}
This section presents the key result, which compares the Para shared memory multi-GPU implementation with hand-coded, python shared memory multi-GPU and python distributed memory TSQR implementations. Figure \ref{fig:parla_overall} presents the overall TSQR runtime while the cost breakdown for the each case presented in figure \ref{fig:parla_breakdown}. We keep the number of columns in the matrix fixed at 100 columns for all the Parla comparisons. For the Parla comparisons, the QR decomposition is done as follows.
\begin{itemize}
    \item Partitioned block qr : performed in GPU $[Q_p,R_p]=qr(A_p)$, only $R_p$ factor is return to the host. 
    \item Gather $R_p$ to $R$, and QR on cpu, $[Q2,R]=qr(R)$
    \item Scatter $Q2$ to $Q2_p$, matrix matrix multiplication on GPU. 
\end{itemize}

\begin{figure}[!tbhp]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                ybar, 
                symbolic x coords={1K,10K,100K,1000K},
                xtick=data,
                width=12cm,
                height=4.5cm,
                grid=major,
                xlabel={number of rows $\rightarrow$}, ylabel={runtime (s) $\rightarrow$},
                %legend pos= north west,
                legend columns=2,
                legend style={at={(0.5,-0.35)},anchor=north},
                bar width=0.2cm,
            ]
            %\addplot[bar shift=-0.6cm,fill=green!30] table[x={Rows},y={total_max}]{dat/frontera_mpi_gpu_v1.dat};
            \addplot[bar shift=-0.4cm,fill=green!70] table[x={Rows},y={total_max}]{dat/frontera_mpi_gpu_v3.dat};
            \addplot[bar shift=-0.2cm,fill=blue!30] table[x={Rows},y={total}]{dat/frontera_sm_gpu.dat};
            \addplot[bar shift=0.0cm,fill=orange!30] table[x={Rows},y={total}]{dat/frontera_parla_gpu_t4.dat};
            \addplot[bar shift=0.2cm,fill=red!30] table[x={Rows},y={total}]{dat/frontera_parla_gpu_t8.dat};
            \addplot[bar shift=0.4cm,fill=yellow!30] table[x={Rows},y={total}]{dat/frontera_parla_gpu_t16.dat};
            \legend{MPI + Cupy, Threads + Cupy, Parla + Cupy (4 blocks), Parla + Cupy (8 blocks), Parla + Cupy (16 blocks) }
        \end{axis}
    \end{tikzpicture}
    \caption{Single node performance for TSQR algorithm implemented using Cupy where the parallelism was achieved by Python MPI, Python threading and Parla framework in Frontera GPU cluster. This experiment matrix column size was fixed at 100 columns.\label{fig:parla_overall}}
\end{figure}
\begin{figure}[!tbhp]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                ybar stacked, 
                symbolic x coords={1K,10K,100K,1000K},
                xtick=data,
                width=12cm,
                height=4.5cm,
                grid=major,
                xlabel={number of rows $\rightarrow$}, ylabel={runtime (s) $\rightarrow$},
                %legend pos= north west,
                legend columns=4,
                legend style={at={(0.5,-0.35)},anchor=north},
                bar width=0.2cm,
            ]
            \addplot[bar shift=-0.35cm,fill=red!30] table[x={Rows},y={kernel_gpu_max}]{dat/frontera_mpi_gpu_v3.dat};
            \addplot[bar shift=-0.35cm,fill=blue!30] table[x={Rows},y={H2D_max}]{dat/frontera_mpi_gpu_v3.dat};
            \addplot[bar shift=-0.35cm,fill=orange!30] table[x={Rows},y={D2H_max}]{dat/frontera_mpi_gpu_v3.dat};
            \addplot[bar shift=-0.35cm,fill=green!30] table[x={Rows},y={mpi_comm_max}]{dat/frontera_mpi_gpu_v3.dat};
            \addplot[bar shift=-0.35cm,fill=yellow!30] table[x={Rows},y expr=\thisrow{total_max}-\thisrow{kernel_gpu_max}-\thisrow{H2D_max}-\thisrow{D2H_max}-\thisrow{mpi_comm_max}]{dat/frontera_mpi_gpu_v3.dat};
            \resetstackedplots
            \addplot[bar shift=-0.1cm,fill=red!30, postaction={pattern=north east lines}] table[x={Rows},y={kernel_gpu_max}]{dat/frontera_sm_gpu.dat};
            \addplot[bar shift=-0.1cm,fill=blue!30, postaction={pattern=north east lines}] table[x={Rows},y={H2D_max}]{dat/frontera_sm_gpu.dat};
            \addplot[bar shift=-0.1cm,fill=orange!30,postaction={pattern=north east lines}] table[x={Rows},y={D2H_max}]{dat/frontera_sm_gpu.dat};
            \addplot[bar shift=-0.1cm,fill=yellow!30,postaction={pattern=north east lines}] table[x={Rows},y expr=\thisrow{total}-\thisrow{kernel_gpu_max}-\thisrow{H2D_max}-\thisrow{D2H_max}]{dat/frontera_sm_gpu.dat};
            \resetstackedplots
            \addplot[bar shift=0.15cm,fill=red!30, postaction={pattern=dots}] table[x={Rows},y expr=\thisrow{qr1}]{dat/frontera_parla_gpu_t4.dat};
            \addplot[bar shift=0.15cm,fill=blue!30, postaction={pattern=dots}] table[x={Rows},y expr=\thisrow{qr2}]{dat/frontera_parla_gpu_t4.dat};
            \addplot[bar shift=0.15cm,fill=green!30, postaction={pattern=dots}] table[x={Rows},y expr=\thisrow{mm}]{dat/frontera_parla_gpu_t4.dat};
            \resetstackedplots
            \addplot[bar shift=0.4cm,fill=red!30, postaction={pattern=grid}] table[x={Rows},y expr=\thisrow{qr1}]{dat/frontera_parla_gpu_t8.dat};
            \addplot[bar shift=0.4cm,fill=blue!30, postaction={pattern=grid}] table[x={Rows},y expr=\thisrow{qr2}]{dat/frontera_parla_gpu_t8.dat};
            \addplot[bar shift=0.4cm,fill=green!30, postaction={pattern=grid}] table[x={Rows},y expr=\thisrow{mm}]{dat/frontera_parla_gpu_t8.dat};
            \legend{GPU(MPI),H2D(MPI),D2H(MPI),COMM(MPI),Other (MPI), GPU(SM),H2D(SM),D2H(SM), Other (SM), Parla 4 (qr1),Parla 4 (qr2), Parla 4 (mm), Parla 8 (q1), Parla 8 (q2), Parla 8 (mm)}
        \end{axis}
    \end{tikzpicture}
    % \caption{Single node performance for TSQR algorithm implemented using Cupy where the parallelism achieved by, Python MPI, Python threading and Parla framework in Frontera GPU cluster. For this experiment matrix column size was fixed at 100 columns.}
    \caption{Overall cost breakdown for MPI + Cupy, Threads + Cupy and Parla using 4 and 8 blocks on 4 GPUs. Note that each block is considered a task. q1 denotes the first QR decomposition on the GPU, q2 denotes the second QR decomposition on the CPU, and mm denotes the GPU matrix-matrix multiplication for final Q matrix computation. \label{fig:parla_breakdown} }
\end{figure}



\subsection{Performance model for shared memory multi-gpu TSQR}
Let $B_{H2D}$, $B_{D2H}$ be the bandwidth for the host to device and device to host data transfers, respectively. Let $QR_{gpu}(m,n)$ and $QR_{cpu}(m,n)$ be the flops/s for the cupy QR and numpy QR decomposition for a given number of rows $m$ and columns $n$. Let $MM_{gpu}(m,n)$ be the flops/s for GPU execution of general matrix-matrix multiplication. Let p be the number of threads (1 per GPU) in the shared memory parallel multi-GPU execution. Then the overall time can be approximated by, 
\begin{align}
    T(m,n,p) &= T(\frac{m}{p}, n)_{qr1,GPU}  + T(np,n)_{qr2,CPU} + T(\frac{m}{p},m)_{mm,GPU} \label{eq:qr_sm}
\end{align} where we can write, 
\begin{equation}
    T(\frac{m}{p}, n)_{qr1,GPU} = \frac{mn}{pB_{H2D}} + \frac{\frac{2mn^2}{p} -\frac{2}{3}n^3}{QR_{gpu}} + \frac{n^2}{B_{D2H}} \label{eq:qr1_sm}
\end{equation}
\begin{equation}
    T(np,n)_{qr2,CPU} = \frac{2pn^3 -\frac{2}{3}n^3}{QR_{cpu}} \label{eq:qr2_sm}
\end{equation}
\begin{equation}
    T(\frac{m}{p},m)_{mm,GPU} = \frac{n^2}{B_{H2D}} + \frac{\frac{mn}{p}(2n-1)}{MM_{gpu}} + \frac{mn}{pH_{D2H}} \label{eq:mm_sm}
\end{equation}

Tables \ref{tb:qr1} and \ref{tb:mm} presents the predicted time for partition QR and matrix-matrix multiplication respectively. For partitioned QR (see Table \ref{tb:qr1}), the difference between presented and reported runtime is almost constant because collecting of $R_p$ matrix from mapped execution is fixed because the size of $R_p$ is independent of the number of rows in the input matrix. 

In contrast with partitioned QR, the final step of the TSQR algorithm is performing a matrix-matrix multiplication to get the final $Q$ matrix. The collection overhead of final $Q_p=Q1_pQ2_p$ matrix from mapped thread pool execution grows with the number of rows in the input matrix, since $Q_p$ is $\frac{m}{p} \times n$ matrix. 


\begin{table}[!tbhp]
    \begin{center}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{ |c|c|c|c|c|c|c|c|c| } 
         \hline
         Rows & Cols & H2D(GB/s) &  D2H(GB/s)  &  QR(Flops/s) &  t1 (s) (runtime) & t2(s) (predicted) & t1-t2 & (t1-t2)/t1\\
         \hline
            1000	& 100 & 2.13 & 	1.12  &	2.35	& 0.014914022	& 0.001948	& 0.012966022 & 0.869384662 \\
            10000	& 100 & 4.77 &  3.95  &	12.23	& 0.017084167	& 0.00443	& 0.012654167 & 0.740695579 \\
            100000	& 100 & 4.71 &  6.08  &	22.21	& 0.045916252	& 0.026449	& 0.019467252 & 0.423973017 \\
            1000000	& 100 & 4.57 &  2.86  &	34.87	& 0.212147208	& 0.184173	& 0.027974208 & 0.131862249 \\
         \hline
        \end{tabular}}
    \end{center}
    \caption{Table presents the predicted QR1 runtime (see (\ref{eq:qr1_sm})) based on the empirically compute bandwidths and flops/s for matrix rows of $\frac{m}{p} \times n$, where $p$ denotes the number of GPUs used for the actual computation. The difference between the predicted and the reported runtime is almost a constant.\label{tb:qr1}}
\end{table}

\begin{table}[!tbhp]
    \begin{center}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{ |c|c|c|c|c|c|c|c|c| } 
         \hline
         Rows & Cols & H2D(GB/s) &  D2H(GB/s)  &  MM(Flops/s) &  t1 (s) (runtime) & t2(s) (predicted) & t1-t2 & (t1-t2)/t1\\
         \hline
            1000	& 100 & 2.29	& 2.40 & 	28.38   &	0.00404992	& 0.000185	& 0.00386492  & 0.954320085\\
            10000	& 100 & 4.85	& 4.97 & 	160.31  &	0.005135891	& 0.000358	& 0.004777891 & 0.930294471\\
            100000	& 100 & 4.98	& 6.66 &    209.65  &	0.011140349	& 0.002726	& 0.008414349 & 0.755303893\\
            1000000	& 100 & 4.21	& 2.91 & 	225.91  &	0.119347156	& 0.030051	& 0.089296156 & 0.748205144\\
         \hline
        \end{tabular}}
    \end{center}
    \caption{Table presents the predicted MM runtime (see (\ref{eq:mm_sm})) based on the empirically compute bandwidths and flops/s for matrix rows of $\frac{m}{p} \times n$, where $p$ denotes the number of GPUs used for the actual computation. The difference between the predicted and the reported runtime grows with $\frac{m}{p}$. \label{tb:mm}}
\end{table}

\subsection{MPI + Parla for TSQR}
This section presents, the weak and strong scaling results for distributed memory TSQR factorization. In the following experiments, the distributed memory parallelism is handled by MPI, and the shared memory based multi-GPU parallelism is achieved through parla.

\begin{figure}[!tbhp]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                ybar stacked, 
                symbolic x coords={1,2,4,8,16},
                xtick=data,
                width=12cm,
                height=4.5cm,
                grid=major,
                xlabel={number of nodes $\rightarrow$}, ylabel={runtime (s) $\rightarrow$},
                %legend pos= north west,
                legend columns=4,
                legend style={at={(0.5,-0.35)},anchor=north},
                bar width=0.2cm,
            ]
            %\addplot[bar shift=-0.6cm,fill=green!30] table[x={Rows},y={total_max}]{dat/frontera_mpi_gpu_v1.dat};
            \addplot[bar shift=-0.25cm,fill=blue!20] table[x={nodes},y={mm_gpu_max}]   {dat/frontera_parla_ws_gz_1K.dat};
            \addplot[bar shift=-0.25cm,fill=orange!20] table[x={nodes},y={qr2_max}]    {dat/frontera_parla_ws_gz_1K.dat};
            \addplot[bar shift=-0.25cm,fill=red!20] table[x={nodes},y={qr_gpu_max}]    {dat/frontera_parla_ws_gz_1K.dat};
            \addplot[bar shift=-0.25cm,fill=yellow!20] table[x={nodes},y={d2h_max}]     {dat/frontera_parla_ws_gz_1K.dat};
            \addplot[bar shift=-0.25cm,fill=cyan!20] table[x={nodes},y={h2d_max}]     {dat/frontera_parla_ws_gz_1K.dat};
            \addplot[bar shift=-0.25cm,fill=green!20] table[x={nodes},y={mpi_comm_max}]{dat/frontera_parla_ws_gz_1K.dat};
            \addplot[bar shift=-0.25cm,fill=black!20] table[x={nodes},y expr=\thisrow{total_max} -\thisrow{mpi_comm_max} -\thisrow{h2d_max}-\thisrow{d2h_max}-\thisrow{qr_gpu_max} -\thisrow{qr2_max} -\thisrow{mm_gpu_max} ]{dat/frontera_parla_ws_gz_1K.dat};
            \resetstackedplotsOne
            \addplot[bar shift=0.0cm,fill=blue!20] table[x={nodes},y={mm_gpu_max}]    {dat/frontera_parla_ws_gz_10K.dat};
            \addplot[bar shift=0.0cm,fill=orange!20] table[x={nodes},y={qr2_max}]     {dat/frontera_parla_ws_gz_10K.dat};
            \addplot[bar shift=0.0cm,fill=red!20] table[x={nodes},y={qr_gpu_max}]     {dat/frontera_parla_ws_gz_10K.dat};
            \addplot[bar shift=0.0cm,fill=yellow!20] table[x={nodes},y={d2h_max}]     {dat/frontera_parla_ws_gz_10K.dat};
            \addplot[bar shift=0.0cm,fill=cyan!20] table[x={nodes},y={h2d_max}]     {dat/frontera_parla_ws_gz_10K.dat};
            \addplot[bar shift=0.0cm,fill=green!20] table[x={nodes},y={mpi_comm_max}] {dat/frontera_parla_ws_gz_10K.dat};
            \addplot[bar shift=0.0cm,fill=black!20] table[x={nodes},y expr=\thisrow{total_max} -\thisrow{mpi_comm_max}  ]{dat/frontera_parla_ws_gz_10K.dat};
            \resetstackedplotsOne
            \addplot[bar shift=0.25cm,fill=blue!20] table[x={nodes},y={mm_gpu_max}]    {dat/frontera_parla_ws_gz_100K.dat};
            \addplot[bar shift=0.25cm,fill=orange!20] table[x={nodes},y={qr2_max}]     {dat/frontera_parla_ws_gz_100K.dat};
            \addplot[bar shift=0.25cm,fill=red!20] table[x={nodes},y={qr_gpu_max}]     {dat/frontera_parla_ws_gz_100K.dat};
            \addplot[bar shift=0.25cm,fill=yellow!20] table[x={nodes},y={d2h_max}]     {dat/frontera_parla_ws_gz_100K.dat};
            \addplot[bar shift=0.25cm,fill=cyan!20] table[x={nodes},y={h2d_max}]       {dat/frontera_parla_ws_gz_100K.dat};
            \addplot[bar shift=0.25cm,fill=green!20] table[x={nodes},y={mpi_comm_max}] {dat/frontera_parla_ws_gz_100K.dat};
            \addplot[bar shift=0.25cm,fill=black!20] table[x={nodes},y expr=\thisrow{total_max} -\thisrow{mpi_comm_max} -\thisrow{h2d_max}-\thisrow{d2h_max}-\thisrow{qr_gpu_max} -\thisrow{qr2_max} -\thisrow{mm_gpu_max} ]{dat/frontera_parla_ws_gz_100K.dat};
            \legend{GEMM(GPU), qr2(CPU), qr1(GPU), D2H, H2D, Comm (MPI), Other }
        \end{axis}
    \end{tikzpicture}
    \caption{ Weak scaling results for three different grain sizes (i.e., number of rows per node) using MPI + Parla based TSQR implementation in Frontera across 64 GPUs. Weak scaling results for grain size 1000, 10K, and 100K are shown in the leftmost bar, middle bar, and the rightmost bar, respectively. \label{fig:parla_ws}
    }
\end{figure}

\begin{figure}[!tbhp]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                ybar stacked, 
                symbolic x coords={1,2,4,8,16},
                xtick=data,
                width=12cm,
                height=4.5cm,
                grid=major,
                xlabel={number of nodes $\rightarrow$}, ylabel={runtime (s) $\rightarrow$},
                %legend pos= north west,
                legend columns=4,
                legend style={at={(0.5,-0.35)},anchor=north},
                bar width=0.2cm,
            ]
            %\addplot[bar shift=-0.6cm,fill=green!30] table[x={Rows},y={total_max}]{dat/frontera_mpi_gpu_v1.dat};
            \addplot[bar shift=-0.25cm,fill=blue!20] table[x={nodes},y={mm_gpu_max}]   {dat/frontera_parla_ws_gz_1K.dat};
            \addplot[bar shift=-0.25cm,fill=orange!20] table[x={nodes},y={qr2_max}]    {dat/frontera_parla_ws_gz_1K.dat};
            \addplot[bar shift=-0.25cm,fill=red!20] table[x={nodes},y={qr_gpu_max}]    {dat/frontera_parla_ws_gz_1K.dat};
            \addplot[bar shift=-0.25cm,fill=yellow!20] table[x={nodes},y={d2h_max}]     {dat/frontera_parla_ws_gz_1K.dat};
            \addplot[bar shift=-0.25cm,fill=cyan!20] table[x={nodes},y={h2d_max}]     {dat/frontera_parla_ws_gz_1K.dat};
            \addplot[bar shift=-0.25cm,fill=green!20] table[x={nodes},y={mpi_comm_max}]{dat/frontera_parla_ws_gz_1K.dat};
            \addplot[bar shift=-0.25cm,fill=black!20] table[x={nodes},y expr=\thisrow{total_max} -\thisrow{mpi_comm_max} -\thisrow{h2d_max}-\thisrow{d2h_max}-\thisrow{qr_gpu_max} -\thisrow{qr2_max} -\thisrow{mm_gpu_max} ]{dat/frontera_parla_ws_gz_1K.dat};
            % \resetstackedplotsOne
            % \addplot[bar shift=0.0cm,fill=blue!20] table[x={nodes},y={mm_gpu_max}]    {dat/frontera_parla_ws_gz_10K.dat};
            % \addplot[bar shift=0.0cm,fill=orange!20] table[x={nodes},y={qr2_max}]     {dat/frontera_parla_ws_gz_10K.dat};
            % \addplot[bar shift=0.0cm,fill=red!20] table[x={nodes},y={qr_gpu_max}]     {dat/frontera_parla_ws_gz_10K.dat};
            % \addplot[bar shift=0.0cm,fill=yellow!20] table[x={nodes},y={d2h_max}]     {dat/frontera_parla_ws_gz_10K.dat};
            % \addplot[bar shift=0.0cm,fill=cyan!20] table[x={nodes},y={h2d_max}]     {dat/frontera_parla_ws_gz_10K.dat};
            % \addplot[bar shift=0.0cm,fill=green!20] table[x={nodes},y={mpi_comm_max}] {dat/frontera_parla_ws_gz_10K.dat};
            % \addplot[bar shift=0.0cm,fill=black!20] table[x={nodes},y expr=\thisrow{total_max} -\thisrow{mpi_comm_max}  ]{dat/frontera_parla_ws_gz_10K.dat};
            % \resetstackedplotsOne
            % \addplot[bar shift=0.25cm,fill=blue!20] table[x={nodes},y={mm_gpu_max}]    {dat/frontera_parla_ws_gz_100K.dat};
            % \addplot[bar shift=0.25cm,fill=orange!20] table[x={nodes},y={qr2_max}]     {dat/frontera_parla_ws_gz_100K.dat};
            % \addplot[bar shift=0.25cm,fill=red!20] table[x={nodes},y={qr_gpu_max}]     {dat/frontera_parla_ws_gz_100K.dat};
            % \addplot[bar shift=0.25cm,fill=yellow!20] table[x={nodes},y={d2h_max}]     {dat/frontera_parla_ws_gz_100K.dat};
            % \addplot[bar shift=0.25cm,fill=cyan!20] table[x={nodes},y={h2d_max}]       {dat/frontera_parla_ws_gz_100K.dat};
            % \addplot[bar shift=0.25cm,fill=green!20] table[x={nodes},y={mpi_comm_max}] {dat/frontera_parla_ws_gz_100K.dat};
            % \addplot[bar shift=0.25cm,fill=black!20] table[x={nodes},y expr=\thisrow{total_max} -\thisrow{mpi_comm_max} -\thisrow{h2d_max}-\thisrow{d2h_max}-\thisrow{qr_gpu_max} -\thisrow{qr2_max} -\thisrow{mm_gpu_max} ]{dat/frontera_parla_ws_gz_100K.dat};
            \legend{GEMM(GPU), qr2(CPU), qr1(GPU), D2H, H2D, Comm (MPI), Other }
        \end{axis}
    \end{tikzpicture}
    \caption{ Weak scaling results for three different grain sizes (i.e., number of rows per node) using MPI + Parla based TSQR implementation in Frontera across 64 GPUs. Weak scaling results for grain size 1000, 10K, and 100K are shown in the leftmost bar, middle bar, and the rightmost bar, respectively. \label{fig:parla_ws}
    }
\end{figure}

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                ybar stacked, 
                symbolic x coords={1,2,4,8,16},
                xtick=data,
                width=12cm,
                height=4.5cm,
                grid=major,
                xlabel={number of nodes $\rightarrow$}, ylabel={runtime (s) $\rightarrow$},
                %legend pos= north west,
                legend columns=4,
                legend style={at={(0.5,-0.35)},anchor=north},
                bar width=0.25cm,
            ]
            %\addplot[bar shift=-0.6cm,fill=green!30] table[x={Rows},y={total_max}]{dat/frontera_mpi_gpu_v1.dat};
            \addplot[bar shift=0.0cm,fill=blue!20] table[x={nodes},y={mm_gpu_max}]   {dat/frontera_parla_ss_1600K.dat};
            \addplot[bar shift=0.0cm,fill=orange!20] table[x={nodes},y={qr2_max}]    {dat/frontera_parla_ss_1600K.dat};
            \addplot[bar shift=0.0cm,fill=red!20] table[x={nodes},y={qr_gpu_max}]    {dat/frontera_parla_ss_1600K.dat};
            \addplot[bar shift=0.0cm,fill=yellow!20] table[x={nodes},y={d2h_max}]    {dat/frontera_parla_ss_1600K.dat};
            \addplot[bar shift=0.0cm,fill=cyan!20] table[x={nodes},y={h2d_max}]      {dat/frontera_parla_ss_1600K.dat};
            \addplot[bar shift=0.0cm,fill=green!20] table[x={nodes},y={mpi_comm_max}]{dat/frontera_parla_ss_1600K.dat};
            \addplot[bar shift=0.0cm,fill=black!20] table[x={nodes},y expr=\thisrow{total_max} -\thisrow{mpi_comm_max} -\thisrow{h2d_max}-\thisrow{d2h_max}-\thisrow{qr_gpu_max} -\thisrow{qr2_max} -\thisrow{mm_gpu_max} ]{dat/frontera_parla_ss_1600K.dat};
            % \resetstackedplotsOne
            % \addplot[bar shift=0.0cm,fill=blue!20] table[x={nodes},y={mm_gpu_max}]    {dat/frontera_parla_ws_gz_10K.dat};
            % \addplot[bar shift=0.0cm,fill=orange!20] table[x={nodes},y={qr2_max}]     {dat/frontera_parla_ws_gz_10K.dat};
            % \addplot[bar shift=0.0cm,fill=red!20] table[x={nodes},y={qr_gpu_max}]     {dat/frontera_parla_ws_gz_10K.dat};
            % \addplot[bar shift=0.0cm,fill=yellow!20] table[x={nodes},y={d2h_max}]     {dat/frontera_parla_ws_gz_10K.dat};
            % \addplot[bar shift=0.0cm,fill=cyan!20] table[x={nodes},y={h2d_max}]     {dat/frontera_parla_ws_gz_10K.dat};
            % \addplot[bar shift=0.0cm,fill=green!20] table[x={nodes},y={mpi_comm_max}] {dat/frontera_parla_ws_gz_10K.dat};
            % \addplot[bar shift=0.0cm,fill=black!20] table[x={nodes},y expr=\thisrow{total_max} -\thisrow{mpi_comm_max}  ]{dat/frontera_parla_ws_gz_10K.dat};
            % \resetstackedplotsOne
            % \addplot[bar shift=0.25cm,fill=blue!20] table[x={nodes},y={mm_gpu_max}]    {dat/frontera_parla_ws_gz_100K.dat};
            % \addplot[bar shift=0.25cm,fill=orange!20] table[x={nodes},y={qr2_max}]     {dat/frontera_parla_ws_gz_100K.dat};
            % \addplot[bar shift=0.25cm,fill=red!20] table[x={nodes},y={qr_gpu_max}]     {dat/frontera_parla_ws_gz_100K.dat};
            % \addplot[bar shift=0.25cm,fill=yellow!20] table[x={nodes},y={d2h_max}]     {dat/frontera_parla_ws_gz_100K.dat};
            % \addplot[bar shift=0.25cm,fill=cyan!20] table[x={nodes},y={h2d_max}]       {dat/frontera_parla_ws_gz_100K.dat};
            % \addplot[bar shift=0.25cm,fill=green!20] table[x={nodes},y={mpi_comm_max}] {dat/frontera_parla_ws_gz_100K.dat};
            % \addplot[bar shift=0.25cm,fill=black!20] table[x={nodes},y expr=\thisrow{total_max} -\thisrow{mpi_comm_max} -\thisrow{h2d_max}-\thisrow{d2h_max}-\thisrow{qr_gpu_max} -\thisrow{qr2_max} -\thisrow{mm_gpu_max} ]{dat/frontera_parla_ws_gz_100K.dat};
            \legend{GEMM(GPU), qr2(CPU), qr1(GPU), D2H, H2D, Comm (MPI), Other }
        \end{axis}
    \end{tikzpicture}
    \caption{ Strong scaling results for fixed matrix size of $1.6M \times 100$ using MPI + Parla based TSQR implementation in Frontera across 64 GPUs. \label{fig:parla_ss}}
\end{figure}


\bibliographystyle{plain}
\bibliography{parla_qr}



\end{document}




